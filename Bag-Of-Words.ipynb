{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from shutil import copyfile\n",
    "import html2text\n",
    "import re\n",
    "import logging\n",
    "import csv\n",
    "import enchant\n",
    "import glob\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree\n",
    "from lxml import html\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labor_discrim_folder = os.path.join(os.path.expanduser('~'), 'Dropbox', 'Indigenous', 'Saved RA Documents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_folder = os.path.join(os.getcwd(), 'html_files6' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Moving HTML Files to local directory\n",
    "filenames = []\n",
    "overwritten = []\n",
    "for index, filename in enumerate(glob.iglob('/home/jason/Dropbox/**/*.h*', recursive=True)):\n",
    "    basename = os.path.basename(filename)\n",
    "    out_filename = \"{}_{}\".format(basename, index)\n",
    "    file_dst = os.path.join(out_folder, out_filename)\n",
    "    copyfile(filename, file_dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unwritten = []\n",
    "# clids = {}\n",
    "# clid_names = []\n",
    "parser = etree.HTMLParser()\n",
    "new_out = os.path.join(os.getcwd(), 'html_files_clid' )\n",
    "for index, file in enumerate(os.listdir(out_folder)):\n",
    "    html_doc = os.path.join(out_folder, file)\n",
    "    try:\n",
    "        tree = etree.parse(html_doc, parser)\n",
    "        clid = tree.xpath('/html/body/section/section/section/div[2]/p[1]//text()')[0].split(\": \",1)[1]\n",
    "        #clids[clid] = ''\n",
    "        #clid_names.append(clid)\n",
    "        out_name = \"{}.html\".format(clid)\n",
    "        out_loc = os.path.join(new_out, out_name)\n",
    "        copyfile(html_doc, out_loc)\n",
    "    except Exception as e:\n",
    "        #clids[file] = e\n",
    "        pass\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtaining list of jbos we both possess\n",
    "joblist_csv = os.path.join(os.getcwd(), 'Relevant_Files', 'Regular_Use', 'JobList.csv')\n",
    "joblist = pd.read_csv(joblist_csv)\n",
    "all_jobs = list(map(lambda s: s.replace(\"\\\"\",\"\"),joblist['clid'].values))\n",
    "all_jobs_set = set(all_jobs)\n",
    "my_jobs = [os.path.splitext(x)[0] for x in os.listdir(new_out)]\n",
    "both_possess = all_jobs_set.intersection(my_jobs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Moving into a Data folder\n",
    "out_path = os.path.join(os.getcwd(), 'Data')\n",
    "for index, file in enumerate(both_possess_paths):\n",
    "    out_loc = os.path.join(out_path, os.path.basename(file))\n",
    "    copyfile(file, out_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the mutually missing HTML files\n",
    "my_jobs_set = set(my_jobs)\n",
    "brig_miss= np.asarray(list(my_jobs_set.difference(all_jobs_set)))\n",
    "me_miss = np.asarray(list(all_jobs_set.difference(my_jobs_set)))\n",
    "with open('missing_html.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['brig_missing'])\n",
    "    for i in brig_miss:\n",
    "        writer.writerow([i])\n",
    "with open('me_miss.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['me_missing'])\n",
    "    for i in me_miss:\n",
    "        writer.writerow([i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_path = os.path.join(os.getcwd(), 'cant_read1.log')\n",
    "logging.basicConfig(filename = log_path, filemode = 'w')\n",
    "logging.info(\"HTML File Could Not be read\")\n",
    "logger = logging.getLogger(\"HTMLOpenError\")\n",
    "\n",
    "corpus_nocheck = []\n",
    "corpus_check = []\n",
    "notaword = []\n",
    "d = enchant.Dict(\"en_US\")\n",
    "# h = html2text.HTML2Text()\n",
    "# h.ignore_links = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_out = os.path.join(os.getcwd(), 'html_files5' )\n",
    "data_dir = os.path.join(os.getcwd(), 'Data')\n",
    "parser = etree.HTMLParser()\n",
    "corpus_body = {}\n",
    "for index, file in enumerate(os.listdir(data_dir)):\n",
    "    html_doc = os.path.join(data_dir, file)\n",
    "    try:\n",
    "        tree = etree.parse(html_doc, parser)\n",
    "        body_text = tree.xpath('//*[@id=\"postingbody\"]//text()')\n",
    "        body_text_str = ' '.join(body_text)\n",
    "        body_text_clean = body_text_str.rstrip().replace(\"\\n\",\"\")\n",
    "        clid = tree.xpath('/html/body/section/section/section/div[2]/p[1]//text()')[0].split(\": \",1)[1]\n",
    "        corpus_body[clid] = body_text_clean\n",
    "#         soup = BeautifulSoup(open(html_doc), \"html.parser\")\n",
    "#         #print(soup.find(id=\"postingbody\").text)\n",
    "#         finding_text = soup.find(id=\"postingbody\")\n",
    "#         text = (finding_text.text).rstrip()\n",
    "#         text_clean = re.sub(r'\\W+', '', text)\n",
    "#         corpus_body.append(text)\n",
    "#         tree = html.fromstring(soup)\n",
    "        \n",
    "    except:\n",
    "        print(index)\n",
    "        logger.exception(html_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.shutdown()\n",
    "#reload(logging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Obtain our bag of words\n",
    "vectorizer = CountVectorizer(stop_words = 'english')\n",
    "X = vectorizer.fit_transform(corpus_body.values())\n",
    "words = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('words.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for i in words:\n",
    "        writer.writerow([i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Remove all non-words and present as dataframe\n",
    "# words_only = np.asarray(words[2493:])\n",
    "# words_only_values = X.toarray()[:,2493:]\n",
    "# bag_of_words_df = pd.DataFrame(words_only_values, columns = words_only, index = corpus_body.keys())\n",
    "# word_counts = bag_of_words_df.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Loading all dropped words\n",
    "# dropped_words = pd.read_csv('dropped_words.csv')\n",
    "# dropped_words_values = dropped_words['dropped_word'].dropna().values\n",
    "# bag_of_words_df.drop(dropped_words_values, axis = 1, inplace = True) #Drop the dropped words\n",
    "# bag_of_words_df.to_csv('bag_of_words_>10.csv') #Save occurrences greater than 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subsetting words only with 100+ occurrences\n",
    "word_counts_df = pd.read_csv('word_counts.csv')\n",
    "word_100 = word_counts_df[word_counts_df['count'] >= 100]['word'].values\n",
    "bag_of_words_100 = bag_of_words_df[word_100]\n",
    "bag_of_words_100.to_csv('bag_of_words_100.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Obtaining a subset of words with more than 100 occurrences\n",
    "# bag_of_words_df\n",
    "# with open('word_occurrences_100+.csv', 'r') as f:\n",
    "#     csvreader = csv.reader(f)\n",
    "#     next(csvreader, None)\n",
    "#     word_choice = [row[0] for row in csvreader]\n",
    "#     f.close()\n",
    "# word_choice\n",
    "# bag_of_words_df_subset = bag_of_words_df[word_choice]\n",
    "# bag_of_words_df_subset.to_csv('bag_of_words_100+.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Acquiring Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Used to label as discriminated if callback for nonminority is 1 and callback for minority is zero\n",
    "def discriminate(row):\n",
    "    if row['callback_non_minority'] == 1.0 and row['callback_minority'] == 0 :\n",
    "        val = 1\n",
    "    else:\n",
    "        val = 0\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Preparing data\n",
    "labor_discrim_data = os.path.join(os.getcwd(),'full_data - Jason.dta')\n",
    "\n",
    "l_discrim_df = pd.read_stata(labor_discrim_data, index_col = 'clid', columns = ['clid', 'callback', 'indigenous'])\n",
    "l_discrim_df.index = l_discrim_df.index.map(str)\n",
    "l_discrim_nonminority = l_discrim_df[l_discrim_df['indigenous'] == 0].rename(index=str, \n",
    "                                                                          columns = {\"callback\": \"callback_non_minority\"}).drop(['indigenous'], axis = 1)\n",
    "l_discrim_minority = l_discrim_df[l_discrim_df['indigenous'] == 1].rename(index=str, \n",
    "                                                                          columns = {\"callback\": \"callback_minority\"}).drop(['indigenous'], axis = 1)\n",
    "l_discrim_fin = l_discrim_nonminority.join(l_discrim_minority, how='outer')\n",
    "l_discrim_fin['discriminated'] = l_discrim_fin.apply(discriminate, axis = 1)\n",
    "l_discrim_fin.to_csv('flattened_discriminated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "discrim_labels = pd.read_csv('flattened_discriminated.csv', index_col = 'clid')\n",
    "bag_of_words_100 = pd.read_csv('bag_of_words_100+.csv', index_col = 'clid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "possessed_ads = set(bag_of_words_100.index.values)\n",
    "all_ads = set(discrim_labels.index.values)\n",
    "labels_to_drop = (all_ads.difference(possessed_ads))\n",
    "word_rows_to_drop = possessed_ads.difference(all_ads)\n",
    "discrim_labels_possessed = discrim_labels.drop(labels_to_drop)\n",
    "bag_of_words_possessed = bag_of_words_100.drop(word_rows_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bag_of_words_labels = bag_of_words_possessed.join(discrim_labels_possessed, how='outer').dropna()\n",
    "bag_of_words_labels.to_csv('labor_discrim_whole.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Getting inconsistency in amount between labeled ADS and the ads I possess\n",
    "labeled_clid = pd.read_csv('flattened_discriminated.csv', index_col = 'clid')\n",
    "clids = set([str(x).split('.')[0] for x in labeled_clid.index.values])\n",
    "data_jobs = [os.path.splitext(x)[0] for x in os.listdir('Data')]\n",
    "data_jobs_set = set(data_jobs)\n",
    "i_possess_labels = clids.intersection(data_jobs)\n",
    "i_miss = clids.difference(data_jobs)\n",
    "label_miss = data_jobs_set.difference(clids)\n",
    "print(\"Labeled Jobs I have: {}\".format(len(i_possess_labels)))\n",
    "print(\"Labeled Jobs I am Missing: {}\".format(len(i_miss)))\n",
    "print(\"Missing Labels for {} jobs\".format(len(label_miss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso-Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bag_of_words_labels = pd.read_csv('labor_discrim_whole.csv', index_col = 'clid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_data = bag_of_words_labels.drop(['callback_non_minority', 'callback_minority', 'discriminated'], axis = 1).as_matrix()\n",
    "y = bag_of_words_labels['discriminated'].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logit_clf = LogisticRegression(penalty='l1')\n",
    "logit_clf.fit(X_data, y) #Fit on data\n",
    "coef_values = logit_clf.coef_[logit_clf.coef_ != 0] #Obtain all non-removed coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(coef_values.shape)\n",
    "print(logit_clf.coef_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_features = bag_of_words_labels.drop(['callback_non_minority', 'callback_minority', 'discriminated'], axis = 1).columns.values\n",
    "word_coef = np.column_stack((X_features,logit_clf.coef_.reshape((740))) )\n",
    "word_coef_nonzero = word_coef[word_coef[:,1] != 0]\n",
    "word_coef_df = pd.DataFrame(word_coef).to_csv('word_weights_all.csv')\n",
    "word_coef_nonzero_df = pd.DataFrame(word_coef_nonzero).to_csv('word_weights_nonzero.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso-Probit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.discrete.discrete_model import Probit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Example: http://www.statsmodels.org/stable/examples/notebooks/generated/discrete_choice_overview.html\n",
    "#Other Examples: http://www.statsmodels.org/stable/examples/index.html#discrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bag_of_words_labels = pd.read_csv('labor_discrim_whole.csv', index_col = 'clid')\n",
    "Bag_of_Words = bag_of_words_labels.drop(['callback_non_minority', 'callback_minority', 'discriminated'], axis = 1).as_matrix()\n",
    "X = sm.add_constant(Bag_of_Words, prepend = False)\n",
    "y = bag_of_words_labels['discriminated'].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probit_mod = sm.Probit(endog, exog_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probit_res = probit_mod.fit_regularized(method = 'l1', disp=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probit_res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probit_mod2 = sm.Probit(endog, exog_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probit_mod2.fit_regularized(method='l1',disp = 1, alpha=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:labordiscrim]",
   "language": "python",
   "name": "conda-env-labordiscrim-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
