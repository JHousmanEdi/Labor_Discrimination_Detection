{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from shutil import copyfile\n",
    "import html2text\n",
    "import re\n",
    "import logging\n",
    "import csv\n",
    "import enchant\n",
    "import glob\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree\n",
    "from lxml import html\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labor_discrim_folder = os.path.join(os.path.expanduser('~'), 'Dropbox', 'Indigenous', 'Saved RA Documents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_folder = os.path.join(os.getcwd(), 'html_files6' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Moving HTML Files to local directory\n",
    "filenames = []\n",
    "overwritten = []\n",
    "for index, filename in enumerate(glob.iglob('/home/jason/Dropbox/**/*.h*', recursive=True)):\n",
    "    basename = os.path.basename(filename)\n",
    "    out_filename = \"{}_{}\".format(basename, index)\n",
    "    file_dst = os.path.join(out_folder, out_filename)\n",
    "    copyfile(filename, file_dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# unwritten = []\n",
    "# clids = {}\n",
    "# clid_names = []\n",
    "parser = etree.HTMLParser()\n",
    "new_out = os.path.join(os.getcwd(), 'html_files_clid' )\n",
    "for index, file in enumerate(os.listdir(out_folder)):\n",
    "    html_doc = os.path.join(out_folder, file)\n",
    "    try:\n",
    "        tree = etree.parse(html_doc, parser)\n",
    "        clid = tree.xpath('/html/body/section/section/section/div[2]/p[1]//text()')[0].split(\": \",1)[1]\n",
    "        #clids[clid] = ''\n",
    "        #clid_names.append(clid)\n",
    "        out_name = \"{}.html\".format(clid)\n",
    "        out_loc = os.path.join(new_out, out_name)\n",
    "        copyfile(html_doc, out_loc)\n",
    "    except Exception as e:\n",
    "        #clids[file] = e\n",
    "        pass\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Obtaining list of jbos we both possess\n",
    "joblist_csv = os.path.join(os.getcwd(), 'Relevant_Files', 'Regular_Use', 'JobList.csv')\n",
    "joblist = pd.read_csv(joblist_csv)\n",
    "all_jobs = list(map(lambda s: s.replace(\"\\\"\",\"\"),joblist['clid'].values))\n",
    "all_jobs_set = set(all_jobs)\n",
    "my_jobs = [os.path.splitext(x)[0] for x in os.listdir(new_out)]\n",
    "both_possess = all_jobs_set.intersection(my_jobs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Moving into a Data folder\n",
    "out_path = os.path.join(os.getcwd(), 'Data')\n",
    "for index, file in enumerate(both_possess_paths):\n",
    "    out_loc = os.path.join(out_path, os.path.basename(file))\n",
    "    copyfile(file, out_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Finding the mutually missing HTML files\n",
    "my_jobs_set = set(my_jobs)\n",
    "brig_miss= np.asarray(list(my_jobs_set.difference(all_jobs_set)))\n",
    "me_miss = np.asarray(list(all_jobs_set.difference(my_jobs_set)))\n",
    "with open('missing_html.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['brig_missing'])\n",
    "    for i in brig_miss:\n",
    "        writer.writerow([i])\n",
    "with open('me_miss.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['me_missing'])\n",
    "    for i in me_miss:\n",
    "        writer.writerow([i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_path = os.path.join(os.getcwd(), 'cant_read1.log')\n",
    "logging.basicConfig(filename = log_path, filemode = 'w')\n",
    "logging.info(\"HTML File Could Not be read\")\n",
    "logger = logging.getLogger(\"HTMLOpenError\")\n",
    "\n",
    "corpus_nocheck = []\n",
    "corpus_check = []\n",
    "notaword = []\n",
    "d = enchant.Dict(\"en_US\")\n",
    "# h = html2text.HTML2Text()\n",
    "# h.ignore_links = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#new_out = os.path.join(os.getcwd(), 'html_files5' )\n",
    "data_dir = os.path.join(os.getcwd(), 'Data')\n",
    "parser = etree.HTMLParser()\n",
    "corpus_body = {}\n",
    "for index, file in enumerate(os.listdir(data_dir)):\n",
    "    html_doc = os.path.join(data_dir, file)\n",
    "    try:\n",
    "        tree = etree.parse(html_doc, parser)\n",
    "        body_text = tree.xpath('//*[@id=\"postingbody\"]//text()')\n",
    "        body_text_str = ' '.join(body_text)\n",
    "        body_text_clean = body_text_str.rstrip().replace(\"\\n\",\"\")\n",
    "        clid = tree.xpath('/html/body/section/section/section/div[2]/p[1]//text()')[0].split(\": \",1)[1]\n",
    "        corpus_body[clid] = body_text_clean\n",
    "#         soup = BeautifulSoup(open(html_doc), \"html.parser\")\n",
    "#         #print(soup.find(id=\"postingbody\").text)\n",
    "#         finding_text = soup.find(id=\"postingbody\")\n",
    "#         text = (finding_text.text).rstrip()\n",
    "#         text_clean = re.sub(r'\\W+', '', text)\n",
    "#         corpus_body.append(text)\n",
    "#         tree = html.fromstring(soup)\n",
    "        \n",
    "    except:\n",
    "        print(index)\n",
    "        logger.exception(html_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.shutdown()\n",
    "#reload(logging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Obtain our bag of words\n",
    "vectorizer = CountVectorizer(stop_words = 'english')\n",
    "X = vectorizer.fit_transform(corpus_body.values())\n",
    "words = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('words.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for i in words:\n",
    "        writer.writerow([i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Remove all non-words and present as dataframe\n",
    "# words_only = np.asarray(words[2493:])\n",
    "# words_only_values = X.toarray()[:,2493:]\n",
    "# bag_of_words_df = pd.DataFrame(words_only_values, columns = words_only, index = corpus_body.keys())\n",
    "# word_counts = bag_of_words_df.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Loading all dropped words\n",
    "# dropped_words = pd.read_csv('dropped_words.csv')\n",
    "# dropped_words_values = dropped_words['dropped_word'].dropna().values\n",
    "# bag_of_words_df.drop(dropped_words_values, axis = 1, inplace = True) #Drop the dropped words\n",
    "# bag_of_words_df.to_csv('bag_of_words_>10.csv') #Save occurrences greater than 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#subsetting words only with 100+ occurrences\n",
    "word_counts_df = pd.read_csv('word_counts.csv')\n",
    "word_100 = word_counts_df[word_counts_df['count'] >= 100]['word'].values\n",
    "bag_of_words_100 = bag_of_words_df[word_100]\n",
    "bag_of_words_100.to_csv('bag_of_words_100.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Obtaining a subset of words with more than 100 occurrences\n",
    "# bag_of_words_df\n",
    "# with open('word_occurrences_100+.csv', 'r') as f:\n",
    "#     csvreader = csv.reader(f)\n",
    "#     next(csvreader, None)\n",
    "#     word_choice = [row[0] for row in csvreader]\n",
    "#     f.close()\n",
    "# word_choice\n",
    "# bag_of_words_df_subset = bag_of_words_df[word_choice]\n",
    "# bag_of_words_df_subset.to_csv('bag_of_words_100+.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Acquiring Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Used to label as discriminated if callback for nonminority is 1 and callback for minority is zero\n",
    "def discriminate(row):\n",
    "    if row['callback_non_minority'] == 1.0 and row['callback_minority'] == 0 :\n",
    "        val = 1\n",
    "    else:\n",
    "        val = 0\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Preparing data\n",
    "labor_discrim_data = os.path.join(os.getcwd(),'full_data - Jason.dta')\n",
    "\n",
    "l_discrim_df = pd.read_stata(labor_discrim_data, index_col = 'clid', columns = ['clid', 'callback', 'indigenous'])\n",
    "l_discrim_df.index = l_discrim_df.index.map(str)\n",
    "l_discrim_nonminority = l_discrim_df[l_discrim_df['indigenous'] == 0].rename(index=str, \n",
    "                                                                          columns = {\"callback\": \"callback_non_minority\"}).drop(['indigenous'], axis = 1)\n",
    "l_discrim_minority = l_discrim_df[l_discrim_df['indigenous'] == 1].rename(index=str, \n",
    "                                                                          columns = {\"callback\": \"callback_minority\"}).drop(['indigenous'], axis = 1)\n",
    "l_discrim_fin = l_discrim_nonminority.join(l_discrim_minority, how='outer')\n",
    "l_discrim_fin['discriminated'] = l_discrim_fin.apply(discriminate, axis = 1)\n",
    "l_discrim_fin.to_csv('flattened_discriminated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "discrim_labels = pd.read_csv('flattened_discriminated.csv', index_col = 'clid')\n",
    "bag_of_words_100 = pd.read_csv('bag_of_words_100+.csv', index_col = 'clid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "possessed_ads = set(bag_of_words_100.index.values)\n",
    "all_ads = set(discrim_labels.index.values)\n",
    "labels_to_drop = (all_ads.difference(possessed_ads))\n",
    "word_rows_to_drop = possessed_ads.difference(all_ads)\n",
    "discrim_labels_possessed = discrim_labels.drop(labels_to_drop)\n",
    "bag_of_words_possessed = bag_of_words_100.drop(word_rows_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bag_of_words_labels = bag_of_words_possessed.join(discrim_labels_possessed, how='outer').dropna()\n",
    "bag_of_words_labels.to_csv('labor_discrim_whole.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Getting inconsistency in amount between labeled ADS and the ads I possess\n",
    "labeled_clid = pd.read_csv('flattened_discriminated.csv', index_col = 'clid')\n",
    "clids = set([str(x).split('.')[0] for x in labeled_clid.index.values])\n",
    "data_jobs = [os.path.splitext(x)[0] for x in os.listdir('Data')]\n",
    "data_jobs_set = set(data_jobs)\n",
    "i_possess_labels = clids.intersection(data_jobs)\n",
    "i_miss = clids.difference(data_jobs)\n",
    "label_miss = data_jobs_set.difference(clids)\n",
    "print(\"Labeled Jobs I have: {}\".format(len(i_possess_labels)))\n",
    "print(\"Labeled Jobs I am Missing: {}\".format(len(i_miss)))\n",
    "print(\"Missing Labels for {} jobs\".format(len(label_miss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso-Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labor_discrim_whole = os.path.join(os.getcwd(), 'Relevant_Files', 'Regular_Use', 'labor_discrim_whole.csv')\n",
    "bag_of_words_labels = pd.read_csv(labor_discrim_whole, index_col = 'clid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_data = bag_of_words_labels.drop(['callback_non_minority', 'callback_minority', 'discriminated'], axis = 1).as_matrix()\n",
    "y = bag_of_words_labels['discriminated'].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logit_clf = LogisticRegression(penalty='l1')\n",
    "logit_clf.fit(X_data, y) #Fit on data\n",
    "coef_values = logit_clf.coef_[logit_clf.coef_ != 0] #Obtain all non-removed coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(470,)\n",
      "(1, 740)\n"
     ]
    }
   ],
   "source": [
    "print(coef_values.shape)\n",
    "print(logit_clf.coef_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_features = bag_of_words_labels.drop(['callback_non_minority', 'callback_minority', 'discriminated'], axis = 1).columns.values\n",
    "word_coef = np.column_stack((X_features,logit_clf.coef_.reshape((740))) )\n",
    "word_coef_nonzero = word_coef[word_coef[:,1] != 0]\n",
    "word_coef_df = pd.DataFrame(word_coef).to_csv('word_weights_all.csv')\n",
    "word_coef_nonzero_df = pd.DataFrame(word_coef_nonzero).to_csv('word_weights_nonzero.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso-Probit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jason/anaconda3/envs/labordiscrim/lib/python3.5/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.discrete.discrete_model import Probit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Example: http://www.statsmodels.org/stable/examples/notebooks/generated/discrete_choice_overview.html\n",
    "#Other Examples: http://www.statsmodels.org/stable/examples/index.html#discrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labor_discrim_whole = os.path.join(os.getcwd(), 'Relevant_Files', 'Regular_Use', 'labor_discrim_whole.csv')\n",
    "bag_of_words_labels = pd.read_csv(labor_discrim_whole, index_col = 'clid')\n",
    "Bag_of_Words = bag_of_words_labels.drop(['callback_non_minority', 'callback_minority', 'discriminated'], axis = 1).as_matrix()\n",
    "X = sm.add_constant(Bag_of_Words, prepend = False)\n",
    "y = bag_of_words_labels['discriminated'].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  3.,  1., ...,  0.,  0.,  1.],\n",
       "       [ 1.,  1.,  0., ...,  0.,  0.,  1.],\n",
       "       [ 0.,  3.,  1., ...,  0.,  0.,  1.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  1., ...,  1.,  0.,  1.],\n",
       "       [ 1.,  1.,  1., ...,  0.,  0.,  1.],\n",
       "       [ 1.,  3.,  1., ...,  0.,  0.,  1.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probit_mod = sm.Probit(y, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration limit exceeded    (Exit mode 9)\n",
      "            Current function value: 0.01132542159751891\n",
      "            Iterations: 1001\n",
      "            Function evaluations: 1008\n",
      "            Gradient evaluations: 1001\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-fdbd0965b03d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprobit_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobit_mod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_regularized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'l1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/labordiscrim/lib/python3.5/site-packages/statsmodels/discrete/discrete_model.py\u001b[0m in \u001b[0;36mfit_regularized\u001b[0;34m(self, start_params, method, maxiter, full_output, disp, callback, alpha, trim_mode, auto_trim_tol, size_trim_tol, qc_tol, **kwargs)\u001b[0m\n\u001b[1;32m    441\u001b[0m                 \u001b[0mfull_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfull_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m                 \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauto_trim_tol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauto_trim_tol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m                 size_trim_tol=size_trim_tol, qc_tol=qc_tol, **kwargs)\n\u001b[0m\u001b[1;32m    444\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'l1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'l1_cvxopt_cp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m             \u001b[0mdiscretefit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL1BinaryResults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbnryfit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/labordiscrim/lib/python3.5/site-packages/statsmodels/discrete/discrete_model.py\u001b[0m in \u001b[0;36mfit_regularized\u001b[0;34m(self, start_params, method, maxiter, full_output, disp, callback, alpha, trim_mode, auto_trim_tol, size_trim_tol, qc_tol, qc_verbose, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxiter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfull_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0mdisp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra_fit_funcs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextra_fit_funcs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m                 cov_params_func=cov_params_func, **kwargs)\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmlefit\u001b[0m \u001b[0;31m# up to subclasses to wrap results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/labordiscrim/lib/python3.5/site-packages/statsmodels/base/model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, start_params, method, maxiter, full_output, disp, fargs, callback, retall, skip_hessian, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0mcov_params_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cov_params_func'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcov_params_func\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m             \u001b[0mHinv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcov_params_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'newton'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfull_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m             \u001b[0mHinv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mretvals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Hessian'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/labordiscrim/lib/python3.5/site-packages/statsmodels/discrete/discrete_model.py\u001b[0m in \u001b[0;36mcov_params_func_l1\u001b[0;34m(self, likelihood_model, xopt, retvals)\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0mH_restricted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnz_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnz_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0;31m# Covariance estimate for the nonzero params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m             \u001b[0mH_restricted_inv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mH_restricted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m             \u001b[0mH_restricted_inv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/labordiscrim/lib/python3.5/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36minv\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'D->D'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'd->d'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0mextobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_linalg_error_extobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 513\u001b[0;31m     \u001b[0mainv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_umath_linalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    514\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mainv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/labordiscrim/lib/python3.5/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36m_raise_linalgerror_singular\u001b[0;34m(err, flag)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Singular matrix\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_nonposdef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "probit_res = probit_mod.fit_regularized(method = 'l1', disp=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probit_res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probit_mod2 = sm.Probit(endog, exog_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probit_mod2.fit_regularized(method='l1',disp = 1, alpha=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:labordiscrim]",
   "language": "python",
   "name": "conda-env-labordiscrim-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
